{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Complete-Workflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Mathis1993/Leaf-Classification-CNN/blob/master/Complete_Workflow.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "1HPCy6Gm4EWA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plant Species Classification using a CNN"
      ]
    },
    {
      "metadata": {
        "id": "oVbjJGpHmi0K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1JyQxgUQk_K4v3LGdeFJlBMABN-WKJt6b\">"
      ]
    },
    {
      "metadata": {
        "id": "ITK-kWQGR0bg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ]
    },
    {
      "metadata": {
        "id": "jI9NdgAC4EWF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the course of learning about AI at [TechLabs](https://tech-labs.de/), we realized a small project to classify plant species using a Convolutional Neural Network (CNN).\n",
        "\n",
        "In 2012, Neeraj Kumar et al. developed [Leafsnap: A Computer Vision System for Automatic Plant Species Identification](https://neerajkumar.org/base/papers/nk_eccv2012_leafsnap.pdf), a mobile app that identifies all 185 tree species in the Northeastern United States using pictures of their leaves. The classification process is based on a computer vision sytem. This system segments the leaf from its background, extracts curvature features of the leaf's contour and classifies it against a dataset containing examples of all the 185 tree species. With this procedure, a top-1 score of about 72% is achieved (meaning that in 72% of cases, the tree species class the computer vision systems assigns the highest probability to is the correct one). \n",
        "\n",
        "Examples of 180 of the 185 different plant species can be examined in the image above.\n",
        "\n",
        "Using this same dataset, that the authors of the paper make available [here](leafsnap.com/dataset/), we asked ourselves if we could possibly beat the traditional computer vision system's performance by implementing the classification task via a CNN. To maximize our learning success, we did not use a pretrained CNN, but built our own network architecture.\n",
        "\n",
        "In this post, we want to give a full overview of all the steps we took to train our CNN to classify Northeastern American tree species.\n",
        "\n",
        "Due to limited computational resources on our private hardware, we used [Google Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb), Google's free cloud service for developing deep Iearning applications on a GPU. We will show the necessary steps for getting ready to work, a detailed tutorial is available on [medium](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d)."
      ]
    },
    {
      "metadata": {
        "id": "R99hBGehRuu4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Google Colaboratory"
      ]
    },
    {
      "metadata": {
        "id": "M5ZXvoKTTnON",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to use Google Colaboratory, we worked in a IPython Notebook located in a Google Drive storage. \n",
        "\n",
        "**Important**: To use the free GPU, go to \"Edit-->Notebook Settings\" and select \"GPU\" as hardware accelerator. \n",
        "\n",
        "Once connected, Drive can be mounted to Google Colab so that files stored in Drive will be available using the code snippet below."
      ]
    },
    {
      "metadata": {
        "id": "EzvucxyW4xWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56aab6f2-f3e9-4f1a-c014-976247130600"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mIOcKZ7BT6iv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we can navigate to the desired directory..."
      ]
    },
    {
      "metadata": {
        "id": "pBJUSjVT4ysL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0700db1a-e50a-4ae5-b3e3-c4472a71e2c4"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/CNN-Model-files"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/CNN-Model-files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6E-GugQiS6PE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "... and start working."
      ]
    },
    {
      "metadata": {
        "id": "dt8soZ4z4EWI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "fboX9qegCPkd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consisting of the following steps:\n",
        "1. Import packages\n",
        "2. Read data frame with information about pictures\n",
        "3. Create numeric labels\n",
        "4. Resize Pictures \n",
        "6. Read pictures as RGB arrays\n",
        "7. Randomize picture order\n",
        "8. Stack picture input into one array\n",
        "9. Normalize input features (pictures) and one-hot encode labels\n",
        "9. Save input features, labels and further info"
      ]
    },
    {
      "metadata": {
        "id": "nyzibeCIUyCH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Import packages"
      ]
    },
    {
      "metadata": {
        "id": "37OIHSaA4EWM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a first step, we import the packages necessary for preprocessing the data. "
      ]
    },
    {
      "metadata": {
        "id": "sC23UEBw4EWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c59450e-9b2c-4e24-e29b-a45a88674097"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PIL\n",
        "#from PIL import ImageOps\n",
        "import imageio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BvPfIomEhY0H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If a package needs to be installed, do:"
      ]
    },
    {
      "metadata": {
        "id": "QtpR-C8Jrt58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "c03c5c8f-2084-4d1f-849c-d2989397c5e7"
      },
      "cell_type": "code",
      "source": [
        "!pip install imageio"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.14.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O1UbVqVHU5Wo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Read data frame with information about pictures"
      ]
    },
    {
      "metadata": {
        "id": "9zp6wtyj_kYC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the dataset, there is a data frame containing information about the pictures. Relevant for us are the columns:\n",
        "- path: path to the individual pictures\n",
        "- species: latin term for each plant\n",
        "- source: picture taken in lab or field"
      ]
    },
    {
      "metadata": {
        "id": "uOUlCZxg4EWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fa35a093-478c-4187-9b3a-156a96925823"
      },
      "cell_type": "code",
      "source": [
        "img_info = pd.read_csv(\"./leafsnap-dataset-images.txt\", sep=\"\\t\")\n",
        "img_info.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_id</th>\n",
              "      <th>image_path</th>\n",
              "      <th>segmented_path</th>\n",
              "      <th>species</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>55497</td>\n",
              "      <td>dataset/images/lab/abies_concolor/ny1157-01-1.jpg</td>\n",
              "      <td>dataset/segmented/lab/abies_concolor/ny1157-01...</td>\n",
              "      <td>Abies concolor</td>\n",
              "      <td>lab</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55498</td>\n",
              "      <td>dataset/images/lab/abies_concolor/ny1157-01-2.jpg</td>\n",
              "      <td>dataset/segmented/lab/abies_concolor/ny1157-01...</td>\n",
              "      <td>Abies concolor</td>\n",
              "      <td>lab</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>55499</td>\n",
              "      <td>dataset/images/lab/abies_concolor/ny1157-01-3.jpg</td>\n",
              "      <td>dataset/segmented/lab/abies_concolor/ny1157-01...</td>\n",
              "      <td>Abies concolor</td>\n",
              "      <td>lab</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>55500</td>\n",
              "      <td>dataset/images/lab/abies_concolor/ny1157-01-4.jpg</td>\n",
              "      <td>dataset/segmented/lab/abies_concolor/ny1157-01...</td>\n",
              "      <td>Abies concolor</td>\n",
              "      <td>lab</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>55501</td>\n",
              "      <td>dataset/images/lab/abies_concolor/ny1157-02-1.jpg</td>\n",
              "      <td>dataset/segmented/lab/abies_concolor/ny1157-02...</td>\n",
              "      <td>Abies concolor</td>\n",
              "      <td>lab</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   file_id                                         image_path  \\\n",
              "0    55497  dataset/images/lab/abies_concolor/ny1157-01-1.jpg   \n",
              "1    55498  dataset/images/lab/abies_concolor/ny1157-01-2.jpg   \n",
              "2    55499  dataset/images/lab/abies_concolor/ny1157-01-3.jpg   \n",
              "3    55500  dataset/images/lab/abies_concolor/ny1157-01-4.jpg   \n",
              "4    55501  dataset/images/lab/abies_concolor/ny1157-02-1.jpg   \n",
              "\n",
              "                                      segmented_path         species source  \n",
              "0  dataset/segmented/lab/abies_concolor/ny1157-01...  Abies concolor    lab  \n",
              "1  dataset/segmented/lab/abies_concolor/ny1157-01...  Abies concolor    lab  \n",
              "2  dataset/segmented/lab/abies_concolor/ny1157-01...  Abies concolor    lab  \n",
              "3  dataset/segmented/lab/abies_concolor/ny1157-01...  Abies concolor    lab  \n",
              "4  dataset/segmented/lab/abies_concolor/ny1157-02...  Abies concolor    lab  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "iuuUfnYwAz_v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We create a new column only holding the filenames and not the whole path to each picture, so that we can assess the filenames for later resizing of the pictures."
      ]
    },
    {
      "metadata": {
        "id": "Eh37RuFvBBkk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new column (empty)\n",
        "img_info[\"filename\"] = None\n",
        "#index of new column\n",
        "index_filename = img_info.columns.get_loc(\"filename\")\n",
        "for i in range(len(img_info)):\n",
        "    img_info.iloc[i, index_filename] = os.path.basename(str(img_info[\"image_path\"][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m6gxVXnMVOFt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Create numeric labels"
      ]
    },
    {
      "metadata": {
        "id": "ErbFWP2zBE67",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we want to have numeric labels instead of the latin term for each plant, so we append another column holding these.\n",
        "- In the data frame, all images of one species are listed consecutively (first the lab images, then the field images)\n",
        "- Therefore we just loop over the dataframe and increment the numeric label whenever we encounter a latin term that differs from the previous one"
      ]
    },
    {
      "metadata": {
        "id": "R7jN-mpeBRDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "807e9f06-904d-4fe6-d18f-8a23bd640555"
      },
      "cell_type": "code",
      "source": [
        "#new column (empty)\n",
        "img_info[\"labels_integer\"] = None\n",
        "#index of new column\n",
        "index_labels_integer = img_info.columns.get_loc(\"labels_integer\")\n",
        "#index of species column\n",
        "index_species = img_info.columns.get_loc(\"species\")\n",
        "#to assign numeric labels starting with 0 for the first species\n",
        "k = 0 \n",
        "for i in range(len(img_info)):\n",
        "    if i == 0:\n",
        "        img_info.iloc[i, index_labels_integer] = k #here, k == 0\n",
        "    if i > 0:\n",
        "        if img_info.iloc[i-1, index_species] == img_info.iloc[i, index_species]:\n",
        "            img_info.iloc[i, index_labels_integer] = k\n",
        "        else:\n",
        "            k += 1\n",
        "            img_info.iloc[i, index_labels_integer] = k\n",
        "img_info.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_id</th>\n",
              "      <th>image_path</th>\n",
              "      <th>segmented_path</th>\n",
              "      <th>species</th>\n",
              "      <th>source</th>\n",
              "      <th>filename</th>\n",
              "      <th>labels_integer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30861</th>\n",
              "      <td>83817</td>\n",
              "      <td>dataset/images/field/ulmus_rubra/1300222828248...</td>\n",
              "      <td>dataset/segmented/field/ulmus_rubra/1300222828...</td>\n",
              "      <td>Ulmus rubra</td>\n",
              "      <td>field</td>\n",
              "      <td>13002228282488.jpg</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30862</th>\n",
              "      <td>83818</td>\n",
              "      <td>dataset/images/field/ulmus_rubra/1300222828261...</td>\n",
              "      <td>dataset/segmented/field/ulmus_rubra/1300222828...</td>\n",
              "      <td>Ulmus rubra</td>\n",
              "      <td>field</td>\n",
              "      <td>13002228282613.jpg</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30863</th>\n",
              "      <td>83819</td>\n",
              "      <td>dataset/images/field/ulmus_rubra/1300222828265...</td>\n",
              "      <td>dataset/segmented/field/ulmus_rubra/1300222828...</td>\n",
              "      <td>Ulmus rubra</td>\n",
              "      <td>field</td>\n",
              "      <td>13002228282655.jpg</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30864</th>\n",
              "      <td>83820</td>\n",
              "      <td>dataset/images/field/ulmus_rubra/1300222828617...</td>\n",
              "      <td>dataset/segmented/field/ulmus_rubra/1300222828...</td>\n",
              "      <td>Ulmus rubra</td>\n",
              "      <td>field</td>\n",
              "      <td>13002228286176.jpg</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30865</th>\n",
              "      <td>83821</td>\n",
              "      <td>dataset/images/field/ulmus_rubra/1300222828044...</td>\n",
              "      <td>dataset/segmented/field/ulmus_rubra/1300222828...</td>\n",
              "      <td>Ulmus rubra</td>\n",
              "      <td>field</td>\n",
              "      <td>13002228280448.jpg</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       file_id                                         image_path  \\\n",
              "30861    83817  dataset/images/field/ulmus_rubra/1300222828248...   \n",
              "30862    83818  dataset/images/field/ulmus_rubra/1300222828261...   \n",
              "30863    83819  dataset/images/field/ulmus_rubra/1300222828265...   \n",
              "30864    83820  dataset/images/field/ulmus_rubra/1300222828617...   \n",
              "30865    83821  dataset/images/field/ulmus_rubra/1300222828044...   \n",
              "\n",
              "                                          segmented_path      species source  \\\n",
              "30861  dataset/segmented/field/ulmus_rubra/1300222828...  Ulmus rubra  field   \n",
              "30862  dataset/segmented/field/ulmus_rubra/1300222828...  Ulmus rubra  field   \n",
              "30863  dataset/segmented/field/ulmus_rubra/1300222828...  Ulmus rubra  field   \n",
              "30864  dataset/segmented/field/ulmus_rubra/1300222828...  Ulmus rubra  field   \n",
              "30865  dataset/segmented/field/ulmus_rubra/1300222828...  Ulmus rubra  field   \n",
              "\n",
              "                 filename  labels_integer  \n",
              "30861  13002228282488.jpg             184  \n",
              "30862  13002228282613.jpg             184  \n",
              "30863  13002228282655.jpg             184  \n",
              "30864  13002228286176.jpg             184  \n",
              "30865  13002228280448.jpg             184  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "YnIVwX0bV9BT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Resize pictures"
      ]
    },
    {
      "metadata": {
        "id": "ixg0Nz1UAnl_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next on the list: Resizing the pictures. This is done by reading the filenames from the data frame, generating a cropped version of the desired size for each picture and saving them to an output directory."
      ]
    },
    {
      "metadata": {
        "id": "vpftFPk84EWn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resizeImage(infile, infile_name_only, output_dir=\"\", size=(1024,768)):\n",
        "  '''\n",
        "  Resize Images to a requestet size (not considerinng aspect ratio)\n",
        "  Input:\n",
        "  - infile: image to be resized (with path)\n",
        "  - infile_name_only: image to be resized (filename only)\n",
        "  - output_dir: where resized images should be stored\n",
        "  - size: output size (tupel of (height, width))\n",
        "  '''\n",
        "  \n",
        "  outfile = os.path.splitext(infile_name_only)[0]\n",
        "  extension = os.path.splitext(infile)[1]\n",
        "  \n",
        "  if infile != outfile:\n",
        "    if not os.path.isfile(output_dir + \"/\" + outfile + extension):\n",
        "      try :\n",
        "        im = PIL.Image.open(infile)\n",
        "        #crops to requested size independt from aspec ratio\n",
        "        im = im.resize(size, PIL.Image.ANTIALIAS) \n",
        "        im.save(output_dir + \"/\" + outfile + extension)\n",
        "      except IOError:\n",
        "        print(\"cannot reduce image for \", infile)\n",
        "\n",
        "output_dir = \"dataset/resized\"\n",
        "size = (256, 256)\n",
        "filenames_dir = list(img_info[\"image_path\"])\n",
        "filenames = list(img_info[\"filename\"])\n",
        "            \n",
        "for i in range(len(filenames)):\n",
        "  resizeImage(filenames_dir[i], filenames[i], output_dir=output_dir, size=size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6cbw_p-BWCqT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Read pictures as RGB arrays"
      ]
    },
    {
      "metadata": {
        "id": "JWwkZnAGBq92",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RGB arrays: All resized images are read as rgb arrays in the order they are listed in the data frame. An RGB array is a numeric representation of a picture, assigning three color values (one for each of the three channels: red, green and blue) to each pixel. From a 64x64 picture, we therefore get a 64x64x3 rgb array.\n"
      ]
    },
    {
      "metadata": {
        "id": "XCQ_MHeXHnvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list_vectors = []\n",
        "\n",
        "for i in range(len(img_info)):\n",
        "    #path to resized images\n",
        "    file = \"dataset/resized\" + \"/\" + img_info.iloc[i, index_filename]\n",
        "    #read as rgb array\n",
        "    img = imageio.imread(file)\n",
        "    #append image vector to list\n",
        "    list_vectors.append(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gVZa62EbWGPr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6. Randomize picture order"
      ]
    },
    {
      "metadata": {
        "id": "w1vvG2s4JUuZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a next step, we want to randomize the order in which the picture data is fed to the CNN to prevent sequence effects, as the data frame comes ordered by species."
      ]
    },
    {
      "metadata": {
        "id": "v6_B6nfsJsde",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#relevant variables\n",
        "label = img_info[\"species\"]\n",
        "source = img_info[\"source\"]\n",
        "label_numeric = img_info[\"labels_integer\"]\n",
        "list_vectors = list_vectors\n",
        "filename = img_info[\"filename\"]\n",
        "\n",
        "#randomization\n",
        "allinfo = list(zip(label, source, label_numeric, list_vectors, filename)) \n",
        "random.shuffle(allinfo) #shuffle\n",
        "label, source, label_numeric, list_vectors, filename = zip(*allinfo) #decompose again\n",
        "img_info_rand = pd.DataFrame({\"filename\":filename, \"label\":label, \"source\":source, \"label_numeric\":label_numeric}) #store picture information in randomized order"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtBoxoW3WLEE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7. Stack picture input into one array"
      ]
    },
    {
      "metadata": {
        "id": "ToAKZFMHJ4vc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, the picture vectors should be stacked vertically into one array, so that each row represents one picture. This array's shape should then be (30866, 64, 64, 3), as we have 30866 pictures, represented in a 64x64x3 array each. "
      ]
    },
    {
      "metadata": {
        "id": "YDszhhPUKGw8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11478703-90bb-4882-870c-80b556f5043b"
      },
      "cell_type": "code",
      "source": [
        "X = np.stack((list_vectors))\n",
        "\n",
        "#transform numeric labels into array\n",
        "Y = np.asarray(label_numeric)\n",
        "\n",
        "print(X.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30866, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UeIF1fBZWTjT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8. Normalize input features (pictures) and one-hot encode labels"
      ]
    },
    {
      "metadata": {
        "id": "42MpuqIhKNrz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we normalize the picture data (dividing by 255, as there are 255 conditions possible for each rgb channel) and one-hot encode the numeric labels. One-hot encoding means to go from a representation where each numeric label is just one number (eg 14 for the 14th of 185 classes, so that we have a vector with 30866 entries for our 30866 pictures), to a representation where in a array each column represents a class and each row a picture. Here, picture number n being of class 14 is represented in the nth row, and only the 14th column contains a one, while all others hold zeros."
      ]
    },
    {
      "metadata": {
        "id": "WrdRFTMpK3Zi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = X/255\n",
        "Y_one_hot = keras.utils.to_categorical(Y, num_classes=185)\n",
        "print(Y.shape, Y_one_hot.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QuFYKtS1qVQx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 9. Save input features, labels and further info"
      ]
    },
    {
      "metadata": {
        "id": "Q2nvWLKqLkbX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can save the input features, labels, and the further information about the pictures (in the randomized order) so that we don't have to repeat all the above steps in the next session."
      ]
    },
    {
      "metadata": {
        "id": "_QJgpYjNLoJY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.savez(\"x_images_arrays\", X)\n",
        "np.savez(\"y_numeric_labels\", Y)\n",
        "img_info_rand.to_csv(\"img_info_rand_256.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0r-U0VPJHp7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "...loading"
      ]
    },
    {
      "metadata": {
        "id": "nOisl_TjJKX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_npz = np.load(\"x_images_arrays.npz\")\n",
        "X = x_npz['arr_0']\n",
        "\n",
        "y_npz = np.load(\"y_numerical_labels.npz\")\n",
        "Y = y_npz['arr_0']\n",
        "\n",
        "img_info_rand = pd.read_csv(\"img_info_rand.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2YR0x_L5LEUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN-Model"
      ]
    },
    {
      "metadata": {
        "id": "rXlBqstePxvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to train a CNN on our dataset, we take the following steps:\n",
        "1. Import packages\n",
        "2. Split data in train-/development-/test-set\n",
        "3. Define the model architecture and compile the model\n",
        "4. Fit the data\n",
        "5. Save the model"
      ]
    },
    {
      "metadata": {
        "id": "EMNj2sRyXy4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Import packages"
      ]
    },
    {
      "metadata": {
        "id": "FA2MyN45Lwn4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we import the necessary packages."
      ]
    },
    {
      "metadata": {
        "id": "wbvAr-XOLDv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZAWMnW_EX1kn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Split data in train-/development-/test-set"
      ]
    },
    {
      "metadata": {
        "id": "iArEjV6XQQ_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we split our picture arrays (and the corresponding labels) into three parts:\n",
        "1. train-set: The data we train the model on (here 80%)\n",
        "2. development-set (dev-set): The data we use to evaluate the model's generalization performance (to new, unseen data) during training (here 10%)\n",
        "3. test-set: The data we want to predict using our trained model (here 10%)\n",
        "  - For further evaluation of results, we isolate labels (latin terms), numeric labels, filenames and source (lab/field) for the test-set"
      ]
    },
    {
      "metadata": {
        "id": "Su3ts69hPwGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba6dddc1-5fd2-445d-c317-f1aef8b71d5d"
      },
      "cell_type": "code",
      "source": [
        "split_train = 0.8 #train 0.8, validate 0.1, test 0.1\n",
        "split_val = 0.9\n",
        "index_train = int(split_train*len(X))\n",
        "index_val = int(split_val*len(X))\n",
        "\n",
        "X_train = X[:index_train]\n",
        "X_val = X[index_train:index_val]\n",
        "X_test = X[index_val:]\n",
        "\n",
        "Y_train = Y[:index_train]\n",
        "Y_val = Y[index_train:index_val]\n",
        "Y_test = Y[index_val:]\n",
        "\n",
        "#for later predictions on test set\n",
        "labels_numeric_test = img_info_rand.loc[index_val:len(X), \"label_numerical\"]\n",
        "labels_test = img_info_rand.loc[index_val:len(X), \"label\"]\n",
        "filenames_test = img_info_rand.loc[index_val:len(X), \"filename\"]\n",
        "source_test = img_info_rand.loc[index_val:len(X), \"source\"]\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24692, 64, 64, 3) (3087, 64, 64, 3) (3087, 64, 64, 3) (24692, 185) (3087, 185) (3087, 185)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t5uh_xBwRhdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Define the model architecture and compile the model"
      ]
    },
    {
      "metadata": {
        "id": "3FLtc7OgfBDM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- **Architecture**\n",
        "\n",
        "The model takes as input pictures of the size (m, 64, 64, 3), so 64x64 colored images (3 rgb channels). \n",
        "\n",
        "In a first convolutional layer, 32 5x5 filters are applied with a stride of 1 (so each filter sliding over the picture in steps of 1 pixel) and no padding. Therefore, the output of this convolutional layer will be (m, 60, 60, 32). \n",
        "\n",
        "This is fed to a max pooling layer, in which a (2,2) window selecting each maximum slides over the input with a stride of 2, collapsing the input to a output of (m, 30, 30 ,32). \n",
        "\n",
        "The next convolutional layer consists of 64 5x5 filters, again with a stride of 1. This generates a output of (m, 26, 26, 64),\n",
        "\n",
        "which through the next pooling layer (specifications as in the first one) is collapsed to a size of (m, 13, 13, 64). \n",
        "\n",
        "The output of this pooling layer is then flattened into a matrix of (10816, m) (13x13x64 = 10816) and \n",
        "\n",
        "fed to a fully connected layer with 1000 nodes, which in turn outputs (1000, m)\n",
        "\n",
        "to the final softmax layer with 185 nodes (185 classes), giving (185, m) as the final output. \n",
        "\n",
        "---\n",
        "\n",
        "- **Activation functions**\n",
        "\n",
        "A Rectified Linear Unit Activation function is used throughout the network (except for the final softmax layer).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **Parameters**\n",
        "\n",
        "Learnable parameters are the values of the filter matrices as well as the weights in the fully connected layers.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **Normalization**\n",
        "\n",
        "Batch Normalization (the output of a previous activation layer is normalized by subtracting the batch mean and dividing by the batch standard deviation) was applied to reduce activations shifting around in a high range, thus making the model more robust and speeding up learning. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **Regularization**\n",
        "\n",
        "After the final max pooling layer and the fully connected layer feeding into the final softmax layer, dropout was implemented to reduce overfitting (in dropout, activations are randomly set to zero with a certain possibility (here 0.7) to prevent the model from only relying on certain high activations resulting from replicating noise in the training data instead of finding a meaningful input to output mapping.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **Optimizer**\n",
        "\n",
        "The model was trained with an Adam optimizer as gradient descent optimization algorithm, which combines moving averages of past weights and squared moving average of past weights (moving average of past weights: At each time step, the average over the last eg 10 weights is calculated and incorporated in the weight updating, so that we not only consider the current timestep, but also timesteps from the past to prevent drastic changes in direction (leading to valleys or local minima) when searching for the minimum in gradient descent, as go into the direction most oft he gradients push as (along the current gradient „flow“)). Hyperparameters controlling the moving averages are beta1 and beta2. Learning rate was set to 0.0001 with a decay of e-8 (to prevent overshooting the global minimum). As loss function, categroical cross entropy cost was used."
      ]
    },
    {
      "metadata": {
        "id": "nG6wSGtvvl8o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3]) #(64, 64, 3)\n",
        "num_classes = 185\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Conv2D(64, (5, 5)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-8, amsgrad=False),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVnShIuieyyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "outputId": "7788f513-d908-4ff7-c5f3-115febe3d239"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 60, 60, 32)        2432      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 60, 60, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 60, 60, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 26, 26, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1000)              10817000  \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 185)               185185    \n",
            "=================================================================\n",
            "Total params: 11,060,265\n",
            "Trainable params: 11,058,073\n",
            "Non-trainable params: 2,192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hvZRONHiX7Q0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Fit the data (and save the best model)"
      ]
    },
    {
      "metadata": {
        "id": "1YuvMqkKweGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 27302
        },
        "outputId": "ef614db3-0428-47cb-fb28-50a1a372f74e"
      },
      "cell_type": "code",
      "source": [
        "# autosave best Model\n",
        "best_model_file = \"leafnet.h5\"\n",
        "best_model = ModelCheckpoint(best_model_file, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "print('Training model...')\n",
        "results = model1.fit(X_train, Y_train, epochs=400, batch_size=512, validation_data=(X_val, Y_val), callbacks=[best_model])\n",
        "print('Traing finished.')\n",
        "\n",
        "print('Loading the best model...')\n",
        "model = load_model(best_model_file)\n",
        "print('Best Model loaded!')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Train on 24692 samples, validate on 3087 samples\n",
            "Epoch 1/400\n",
            "24692/24692 [==============================] - 13s 524us/step - loss: 6.9255 - acc: 0.0180 - val_loss: 4.1561 - val_acc: 0.1318\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.15608, saving model to leafnet_batch_512.h5\n",
            "Epoch 2/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 5.6699 - acc: 0.0577 - val_loss: 3.4366 - val_acc: 0.2310\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.15608 to 3.43664, saving model to leafnet_batch_512.h5\n",
            "Epoch 3/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 4.9649 - acc: 0.0958 - val_loss: 2.9717 - val_acc: 0.3094\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.43664 to 2.97165, saving model to leafnet_batch_512.h5\n",
            "Epoch 4/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 4.4661 - acc: 0.1309 - val_loss: 2.6748 - val_acc: 0.3732\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.97165 to 2.67475, saving model to leafnet_batch_512.h5\n",
            "Epoch 5/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 4.0674 - acc: 0.1709 - val_loss: 2.4829 - val_acc: 0.3997\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.67475 to 2.48295, saving model to leafnet_batch_512.h5\n",
            "Epoch 6/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 3.7969 - acc: 0.1978 - val_loss: 2.3363 - val_acc: 0.4357\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.48295 to 2.33631, saving model to leafnet_batch_512.h5\n",
            "Epoch 7/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 3.5743 - acc: 0.2215 - val_loss: 2.2398 - val_acc: 0.4393\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.33631 to 2.23978, saving model to leafnet_batch_512.h5\n",
            "Epoch 8/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 3.3554 - acc: 0.2540 - val_loss: 2.1069 - val_acc: 0.4791\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.23978 to 2.10694, saving model to leafnet_batch_512.h5\n",
            "Epoch 9/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 3.1981 - acc: 0.2696 - val_loss: 2.0044 - val_acc: 0.4969\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.10694 to 2.00436, saving model to leafnet_batch_512.h5\n",
            "Epoch 10/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 3.0472 - acc: 0.2927 - val_loss: 1.9255 - val_acc: 0.5219\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.00436 to 1.92554, saving model to leafnet_batch_512.h5\n",
            "Epoch 11/400\n",
            "24692/24692 [==============================] - 11s 449us/step - loss: 2.9287 - acc: 0.3071 - val_loss: 1.8525 - val_acc: 0.5348\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.92554 to 1.85253, saving model to leafnet_batch_512.h5\n",
            "Epoch 12/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 2.8008 - acc: 0.3314 - val_loss: 1.7860 - val_acc: 0.5478\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.85253 to 1.78603, saving model to leafnet_batch_512.h5\n",
            "Epoch 13/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 2.6744 - acc: 0.3483 - val_loss: 1.7457 - val_acc: 0.5559\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.78603 to 1.74566, saving model to leafnet_batch_512.h5\n",
            "Epoch 14/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 2.5773 - acc: 0.3645 - val_loss: 1.6924 - val_acc: 0.5549\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.74566 to 1.69245, saving model to leafnet_batch_512.h5\n",
            "Epoch 15/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 2.4861 - acc: 0.3807 - val_loss: 1.6141 - val_acc: 0.5837\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.69245 to 1.61414, saving model to leafnet_batch_512.h5\n",
            "Epoch 16/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 2.4065 - acc: 0.3936 - val_loss: 1.5760 - val_acc: 0.5928\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.61414 to 1.57598, saving model to leafnet_batch_512.h5\n",
            "Epoch 17/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 2.3408 - acc: 0.4061 - val_loss: 1.5188 - val_acc: 0.6067\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.57598 to 1.51881, saving model to leafnet_batch_512.h5\n",
            "Epoch 18/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 2.2640 - acc: 0.4182 - val_loss: 1.5025 - val_acc: 0.6103\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.51881 to 1.50252, saving model to leafnet_batch_512.h5\n",
            "Epoch 19/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 2.1621 - acc: 0.4383 - val_loss: 1.4607 - val_acc: 0.6087\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.50252 to 1.46074, saving model to leafnet_batch_512.h5\n",
            "Epoch 20/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 2.1138 - acc: 0.4497 - val_loss: 1.4191 - val_acc: 0.6190\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.46074 to 1.41910, saving model to leafnet_batch_512.h5\n",
            "Epoch 21/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 2.0515 - acc: 0.4602 - val_loss: 1.4021 - val_acc: 0.6246\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.41910 to 1.40212, saving model to leafnet_batch_512.h5\n",
            "Epoch 22/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.9993 - acc: 0.4696 - val_loss: 1.3709 - val_acc: 0.6352\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.40212 to 1.37087, saving model to leafnet_batch_512.h5\n",
            "Epoch 23/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.9559 - acc: 0.4777 - val_loss: 1.3671 - val_acc: 0.6249\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.37087 to 1.36706, saving model to leafnet_batch_512.h5\n",
            "Epoch 24/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 1.8871 - acc: 0.4936 - val_loss: 1.3116 - val_acc: 0.6404\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.36706 to 1.31163, saving model to leafnet_batch_512.h5\n",
            "Epoch 25/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 1.8330 - acc: 0.5070 - val_loss: 1.2742 - val_acc: 0.6540\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.31163 to 1.27417, saving model to leafnet_batch_512.h5\n",
            "Epoch 26/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 1.8021 - acc: 0.5160 - val_loss: 1.2602 - val_acc: 0.6560\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.27417 to 1.26017, saving model to leafnet_batch_512.h5\n",
            "Epoch 27/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.7481 - acc: 0.5218 - val_loss: 1.2303 - val_acc: 0.6663\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.26017 to 1.23028, saving model to leafnet_batch_512.h5\n",
            "Epoch 28/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 1.7058 - acc: 0.5315 - val_loss: 1.2473 - val_acc: 0.6537\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.23028\n",
            "Epoch 29/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.6807 - acc: 0.5373 - val_loss: 1.2052 - val_acc: 0.6741\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.23028 to 1.20521, saving model to leafnet_batch_512.h5\n",
            "Epoch 30/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.6139 - acc: 0.5509 - val_loss: 1.1685 - val_acc: 0.6777\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.20521 to 1.16853, saving model to leafnet_batch_512.h5\n",
            "Epoch 31/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 1.5949 - acc: 0.5572 - val_loss: 1.1469 - val_acc: 0.6855\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.16853 to 1.14692, saving model to leafnet_batch_512.h5\n",
            "Epoch 32/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 1.5538 - acc: 0.5663 - val_loss: 1.1082 - val_acc: 0.6978\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.14692 to 1.10824, saving model to leafnet_batch_512.h5\n",
            "Epoch 33/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.5109 - acc: 0.5750 - val_loss: 1.1151 - val_acc: 0.6932\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.10824\n",
            "Epoch 34/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.4933 - acc: 0.5769 - val_loss: 1.1434 - val_acc: 0.6793\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.10824\n",
            "Epoch 35/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.4618 - acc: 0.5880 - val_loss: 1.1363 - val_acc: 0.6851\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.10824\n",
            "Epoch 36/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.4165 - acc: 0.5976 - val_loss: 1.0727 - val_acc: 0.7007\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.10824 to 1.07266, saving model to leafnet_batch_512.h5\n",
            "Epoch 37/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 1.4026 - acc: 0.6031 - val_loss: 1.0683 - val_acc: 0.6968\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.07266 to 1.06826, saving model to leafnet_batch_512.h5\n",
            "Epoch 38/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.3650 - acc: 0.6145 - val_loss: 1.0639 - val_acc: 0.6923\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.06826 to 1.06394, saving model to leafnet_batch_512.h5\n",
            "Epoch 39/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.3399 - acc: 0.6182 - val_loss: 1.0639 - val_acc: 0.6916\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.06394\n",
            "Epoch 40/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 1.3293 - acc: 0.6203 - val_loss: 1.0203 - val_acc: 0.7094\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.06394 to 1.02031, saving model to leafnet_batch_512.h5\n",
            "Epoch 41/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.3039 - acc: 0.6251 - val_loss: 1.0323 - val_acc: 0.7085\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.02031\n",
            "Epoch 42/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 1.2618 - acc: 0.6364 - val_loss: 0.9989 - val_acc: 0.7243\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.02031 to 0.99887, saving model to leafnet_batch_512.h5\n",
            "Epoch 43/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.2432 - acc: 0.6361 - val_loss: 0.9679 - val_acc: 0.7298\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.99887 to 0.96791, saving model to leafnet_batch_512.h5\n",
            "Epoch 44/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 1.2368 - acc: 0.6458 - val_loss: 0.9805 - val_acc: 0.7211\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.96791\n",
            "Epoch 45/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 1.2026 - acc: 0.6510 - val_loss: 0.9628 - val_acc: 0.7227\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.96791 to 0.96275, saving model to leafnet_batch_512.h5\n",
            "Epoch 46/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.1753 - acc: 0.6591 - val_loss: 0.9372 - val_acc: 0.7295\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.96275 to 0.93722, saving model to leafnet_batch_512.h5\n",
            "Epoch 47/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.1551 - acc: 0.6596 - val_loss: 0.9119 - val_acc: 0.7379\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.93722 to 0.91190, saving model to leafnet_batch_512.h5\n",
            "Epoch 48/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.1328 - acc: 0.6678 - val_loss: 0.9007 - val_acc: 0.7483\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.91190 to 0.90066, saving model to leafnet_batch_512.h5\n",
            "Epoch 49/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 1.1195 - acc: 0.6737 - val_loss: 0.9104 - val_acc: 0.7447\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.90066\n",
            "Epoch 50/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.0942 - acc: 0.6765 - val_loss: 0.8930 - val_acc: 0.7467\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.90066 to 0.89297, saving model to leafnet_batch_512.h5\n",
            "Epoch 51/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 1.0707 - acc: 0.6863 - val_loss: 0.8898 - val_acc: 0.7402\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.89297 to 0.88982, saving model to leafnet_batch_512.h5\n",
            "Epoch 52/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.0653 - acc: 0.6863 - val_loss: 0.8742 - val_acc: 0.7499\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.88982 to 0.87418, saving model to leafnet_batch_512.h5\n",
            "Epoch 53/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 1.0456 - acc: 0.6896 - val_loss: 0.8948 - val_acc: 0.7502\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.87418\n",
            "Epoch 54/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.0082 - acc: 0.6999 - val_loss: 0.8291 - val_acc: 0.7668\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.87418 to 0.82907, saving model to leafnet_batch_512.h5\n",
            "Epoch 55/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 1.0040 - acc: 0.7006 - val_loss: 0.8487 - val_acc: 0.7590\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.82907\n",
            "Epoch 56/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.9926 - acc: 0.7050 - val_loss: 0.8753 - val_acc: 0.7464\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.82907\n",
            "Epoch 57/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.9778 - acc: 0.7084 - val_loss: 0.8413 - val_acc: 0.7658\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.82907\n",
            "Epoch 58/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.9627 - acc: 0.7120 - val_loss: 0.8650 - val_acc: 0.7538\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.82907\n",
            "Epoch 59/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.9473 - acc: 0.7155 - val_loss: 0.8145 - val_acc: 0.7613\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.82907 to 0.81450, saving model to leafnet_batch_512.h5\n",
            "Epoch 60/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.9320 - acc: 0.7193 - val_loss: 0.8039 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.81450 to 0.80385, saving model to leafnet_batch_512.h5\n",
            "Epoch 61/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.9086 - acc: 0.7269 - val_loss: 0.8070 - val_acc: 0.7681\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.80385\n",
            "Epoch 62/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.8968 - acc: 0.7292 - val_loss: 0.8445 - val_acc: 0.7596\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.80385\n",
            "Epoch 63/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.8841 - acc: 0.7332 - val_loss: 0.7760 - val_acc: 0.7771\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.80385 to 0.77597, saving model to leafnet_batch_512.h5\n",
            "Epoch 64/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.8751 - acc: 0.7356 - val_loss: 0.8085 - val_acc: 0.7684\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.77597\n",
            "Epoch 65/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.8590 - acc: 0.7413 - val_loss: 0.8730 - val_acc: 0.7470\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.77597\n",
            "Epoch 66/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.8442 - acc: 0.7435 - val_loss: 0.8071 - val_acc: 0.7681\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.77597\n",
            "Epoch 67/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.8451 - acc: 0.7445 - val_loss: 0.7470 - val_acc: 0.7843\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.77597 to 0.74699, saving model to leafnet_batch_512.h5\n",
            "Epoch 68/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.8275 - acc: 0.7488 - val_loss: 0.7537 - val_acc: 0.7817\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.74699\n",
            "Epoch 69/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.8097 - acc: 0.7528 - val_loss: 0.7310 - val_acc: 0.7872\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.74699 to 0.73102, saving model to leafnet_batch_512.h5\n",
            "Epoch 70/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.7878 - acc: 0.7615 - val_loss: 0.7430 - val_acc: 0.7794\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.73102\n",
            "Epoch 71/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.7876 - acc: 0.7562 - val_loss: 0.7978 - val_acc: 0.7661\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.73102\n",
            "Epoch 72/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.7821 - acc: 0.7597 - val_loss: 0.7294 - val_acc: 0.7852\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.73102 to 0.72943, saving model to leafnet_batch_512.h5\n",
            "Epoch 73/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.7684 - acc: 0.7674 - val_loss: 0.7126 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.72943 to 0.71264, saving model to leafnet_batch_512.h5\n",
            "Epoch 74/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.7531 - acc: 0.7710 - val_loss: 0.7088 - val_acc: 0.7898\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.71264 to 0.70879, saving model to leafnet_batch_512.h5\n",
            "Epoch 75/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.7387 - acc: 0.7742 - val_loss: 0.7237 - val_acc: 0.7904\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.70879\n",
            "Epoch 76/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.7346 - acc: 0.7760 - val_loss: 0.6952 - val_acc: 0.7962\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.70879 to 0.69515, saving model to leafnet_batch_512.h5\n",
            "Epoch 77/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.7254 - acc: 0.7783 - val_loss: 0.8648 - val_acc: 0.7438\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.69515\n",
            "Epoch 78/400\n",
            "24692/24692 [==============================] - 11s 448us/step - loss: 0.7232 - acc: 0.7791 - val_loss: 0.6969 - val_acc: 0.7914\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.69515\n",
            "Epoch 79/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.7038 - acc: 0.7860 - val_loss: 0.7098 - val_acc: 0.7894\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.69515\n",
            "Epoch 80/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.6910 - acc: 0.7868 - val_loss: 0.8961 - val_acc: 0.7201\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.69515\n",
            "Epoch 81/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.6810 - acc: 0.7882 - val_loss: 0.6557 - val_acc: 0.8092\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.69515 to 0.65572, saving model to leafnet_batch_512.h5\n",
            "Epoch 82/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.6826 - acc: 0.7894 - val_loss: 0.6747 - val_acc: 0.7992\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.65572\n",
            "Epoch 83/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.6679 - acc: 0.7917 - val_loss: 0.7186 - val_acc: 0.7836\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.65572\n",
            "Epoch 84/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.6579 - acc: 0.7965 - val_loss: 0.6768 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.65572\n",
            "Epoch 85/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.6525 - acc: 0.8002 - val_loss: 0.6562 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.65572\n",
            "Epoch 86/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.6355 - acc: 0.8035 - val_loss: 0.6794 - val_acc: 0.8014\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.65572\n",
            "Epoch 87/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.6372 - acc: 0.8020 - val_loss: 0.7058 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.65572\n",
            "Epoch 88/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.6240 - acc: 0.8060 - val_loss: 0.6565 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.65572\n",
            "Epoch 89/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.6072 - acc: 0.8124 - val_loss: 0.7444 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.65572\n",
            "Epoch 90/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.6068 - acc: 0.8101 - val_loss: 0.6258 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.65572 to 0.62576, saving model to leafnet_batch_512.h5\n",
            "Epoch 91/400\n",
            "24692/24692 [==============================] - 11s 457us/step - loss: 0.6001 - acc: 0.8144 - val_loss: 0.6863 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.62576\n",
            "Epoch 92/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.5969 - acc: 0.8148 - val_loss: 0.6218 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.62576 to 0.62178, saving model to leafnet_batch_512.h5\n",
            "Epoch 93/400\n",
            "24692/24692 [==============================] - 11s 457us/step - loss: 0.5780 - acc: 0.8206 - val_loss: 0.6303 - val_acc: 0.8154\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.62178\n",
            "Epoch 94/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.5781 - acc: 0.8195 - val_loss: 0.6554 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.62178\n",
            "Epoch 95/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.5689 - acc: 0.8243 - val_loss: 0.7016 - val_acc: 0.7787\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.62178\n",
            "Epoch 96/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.5657 - acc: 0.8248 - val_loss: 0.5941 - val_acc: 0.8231\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.62178 to 0.59406, saving model to leafnet_batch_512.h5\n",
            "Epoch 97/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.5460 - acc: 0.8295 - val_loss: 0.6222 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.59406\n",
            "Epoch 98/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.5435 - acc: 0.8291 - val_loss: 0.6260 - val_acc: 0.8163\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.59406\n",
            "Epoch 99/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.5451 - acc: 0.8317 - val_loss: 0.5954 - val_acc: 0.8228\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.59406\n",
            "Epoch 100/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.5412 - acc: 0.8279 - val_loss: 0.6596 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.59406\n",
            "Epoch 101/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.5191 - acc: 0.8380 - val_loss: 0.5935 - val_acc: 0.8228\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.59406 to 0.59351, saving model to leafnet_batch_512.h5\n",
            "Epoch 102/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.5154 - acc: 0.8376 - val_loss: 0.6093 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.59351\n",
            "Epoch 103/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.5306 - acc: 0.8343 - val_loss: 0.5797 - val_acc: 0.8244\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.59351 to 0.57971, saving model to leafnet_batch_512.h5\n",
            "Epoch 104/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.5024 - acc: 0.8434 - val_loss: 0.6017 - val_acc: 0.8218\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.57971\n",
            "Epoch 105/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.5026 - acc: 0.8434 - val_loss: 0.5835 - val_acc: 0.8254\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.57971\n",
            "Epoch 106/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 0.4915 - acc: 0.8453 - val_loss: 0.6013 - val_acc: 0.8251\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.57971\n",
            "Epoch 107/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.4982 - acc: 0.8441 - val_loss: 0.6148 - val_acc: 0.8205\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.57971\n",
            "Epoch 108/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4887 - acc: 0.8457 - val_loss: 0.6101 - val_acc: 0.8231\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.57971\n",
            "Epoch 109/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4790 - acc: 0.8504 - val_loss: 0.6058 - val_acc: 0.8228\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.57971\n",
            "Epoch 110/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4806 - acc: 0.8484 - val_loss: 0.5893 - val_acc: 0.8283\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.57971\n",
            "Epoch 111/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4599 - acc: 0.8535 - val_loss: 0.6834 - val_acc: 0.7946\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.57971\n",
            "Epoch 112/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4610 - acc: 0.8523 - val_loss: 0.6090 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.57971\n",
            "Epoch 113/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.4615 - acc: 0.8529 - val_loss: 0.6997 - val_acc: 0.7804\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.57971\n",
            "Epoch 114/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.4564 - acc: 0.8558 - val_loss: 0.6391 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.57971\n",
            "Epoch 115/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4408 - acc: 0.8592 - val_loss: 0.5623 - val_acc: 0.8348\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.57971 to 0.56234, saving model to leafnet_batch_512.h5\n",
            "Epoch 116/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.4363 - acc: 0.8639 - val_loss: 0.6269 - val_acc: 0.8160\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.56234\n",
            "Epoch 117/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.4390 - acc: 0.8626 - val_loss: 0.5492 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.56234 to 0.54917, saving model to leafnet_batch_512.h5\n",
            "Epoch 118/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4343 - acc: 0.8629 - val_loss: 0.5737 - val_acc: 0.8286\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.54917\n",
            "Epoch 119/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4296 - acc: 0.8633 - val_loss: 0.5721 - val_acc: 0.8241\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.54917\n",
            "Epoch 120/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.4299 - acc: 0.8619 - val_loss: 0.5534 - val_acc: 0.8367\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.54917\n",
            "Epoch 121/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.4222 - acc: 0.8656 - val_loss: 0.5818 - val_acc: 0.8306\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.54917\n",
            "Epoch 122/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4134 - acc: 0.8703 - val_loss: 0.6082 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.54917\n",
            "Epoch 123/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4031 - acc: 0.8714 - val_loss: 0.5694 - val_acc: 0.8290\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.54917\n",
            "Epoch 124/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.4089 - acc: 0.8704 - val_loss: 0.5513 - val_acc: 0.8397\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.54917\n",
            "Epoch 125/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3964 - acc: 0.8739 - val_loss: 0.5408 - val_acc: 0.8351\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.54917 to 0.54082, saving model to leafnet_batch_512.h5\n",
            "Epoch 126/400\n",
            "24692/24692 [==============================] - 11s 457us/step - loss: 0.3954 - acc: 0.8735 - val_loss: 0.5381 - val_acc: 0.8413\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.54082 to 0.53812, saving model to leafnet_batch_512.h5\n",
            "Epoch 127/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.3845 - acc: 0.8792 - val_loss: 0.7053 - val_acc: 0.7894\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.53812\n",
            "Epoch 128/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3873 - acc: 0.8762 - val_loss: 0.6482 - val_acc: 0.8034\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.53812\n",
            "Epoch 129/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.3790 - acc: 0.8787 - val_loss: 0.5516 - val_acc: 0.8361\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.53812\n",
            "Epoch 130/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3834 - acc: 0.8805 - val_loss: 0.6093 - val_acc: 0.8192\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.53812\n",
            "Epoch 131/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.3854 - acc: 0.8765 - val_loss: 0.5359 - val_acc: 0.8351\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.53812 to 0.53592, saving model to leafnet_batch_512.h5\n",
            "Epoch 132/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.3715 - acc: 0.8833 - val_loss: 0.5338 - val_acc: 0.8400\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.53592 to 0.53385, saving model to leafnet_batch_512.h5\n",
            "Epoch 133/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 0.3695 - acc: 0.8821 - val_loss: 0.5621 - val_acc: 0.8270\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.53385\n",
            "Epoch 134/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.3670 - acc: 0.8849 - val_loss: 0.5407 - val_acc: 0.8387\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.53385\n",
            "Epoch 135/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.3645 - acc: 0.8853 - val_loss: 0.5235 - val_acc: 0.8448\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.53385 to 0.52347, saving model to leafnet_batch_512.h5\n",
            "Epoch 136/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3536 - acc: 0.8877 - val_loss: 0.5453 - val_acc: 0.8403\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.52347\n",
            "Epoch 137/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.3582 - acc: 0.8853 - val_loss: 0.5320 - val_acc: 0.8416\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.52347\n",
            "Epoch 138/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.3520 - acc: 0.8875 - val_loss: 0.5530 - val_acc: 0.8367\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.52347\n",
            "Epoch 139/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.3460 - acc: 0.8896 - val_loss: 0.5466 - val_acc: 0.8371\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.52347\n",
            "Epoch 140/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.3391 - acc: 0.8915 - val_loss: 0.5077 - val_acc: 0.8448\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.52347 to 0.50771, saving model to leafnet_batch_512.h5\n",
            "Epoch 141/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3358 - acc: 0.8923 - val_loss: 0.5190 - val_acc: 0.8458\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.50771\n",
            "Epoch 142/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3311 - acc: 0.8935 - val_loss: 0.5950 - val_acc: 0.8189\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.50771\n",
            "Epoch 143/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.3322 - acc: 0.8944 - val_loss: 0.5261 - val_acc: 0.8397\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.50771\n",
            "Epoch 144/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.3264 - acc: 0.8953 - val_loss: 0.5007 - val_acc: 0.8465\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.50771 to 0.50068, saving model to leafnet_batch_512.h5\n",
            "Epoch 145/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3174 - acc: 0.9003 - val_loss: 0.7251 - val_acc: 0.7758\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.50068\n",
            "Epoch 146/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3278 - acc: 0.8945 - val_loss: 0.5471 - val_acc: 0.8354\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.50068\n",
            "Epoch 147/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3193 - acc: 0.8970 - val_loss: 0.5115 - val_acc: 0.8452\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.50068\n",
            "Epoch 148/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.3144 - acc: 0.8988 - val_loss: 0.5022 - val_acc: 0.8513\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.50068\n",
            "Epoch 149/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.3087 - acc: 0.9010 - val_loss: 0.5041 - val_acc: 0.8484\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.50068\n",
            "Epoch 150/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.3142 - acc: 0.8993 - val_loss: 0.5274 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.50068\n",
            "Epoch 151/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.3114 - acc: 0.9013 - val_loss: 0.4872 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.50068 to 0.48723, saving model to leafnet_batch_512.h5\n",
            "Epoch 152/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.3075 - acc: 0.9013 - val_loss: 0.4936 - val_acc: 0.8533\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.48723\n",
            "Epoch 153/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2986 - acc: 0.9045 - val_loss: 0.5602 - val_acc: 0.8257\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.48723\n",
            "Epoch 154/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2990 - acc: 0.9023 - val_loss: 0.6008 - val_acc: 0.8150\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.48723\n",
            "Epoch 155/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2937 - acc: 0.9061 - val_loss: 0.4940 - val_acc: 0.8536\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.48723\n",
            "Epoch 156/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2851 - acc: 0.9102 - val_loss: 0.5294 - val_acc: 0.8439\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.48723\n",
            "Epoch 157/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.2860 - acc: 0.9088 - val_loss: 0.5603 - val_acc: 0.8325\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.48723\n",
            "Epoch 158/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2882 - acc: 0.9077 - val_loss: 0.5509 - val_acc: 0.8280\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.48723\n",
            "Epoch 159/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2851 - acc: 0.9069 - val_loss: 0.5069 - val_acc: 0.8487\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.48723\n",
            "Epoch 160/400\n",
            "24692/24692 [==============================] - 11s 449us/step - loss: 0.2764 - acc: 0.9087 - val_loss: 0.5412 - val_acc: 0.8403\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.48723\n",
            "Epoch 161/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.2848 - acc: 0.9089 - val_loss: 0.4752 - val_acc: 0.8575\n",
            "\n",
            "Epoch 00161: val_loss improved from 0.48723 to 0.47518, saving model to leafnet_batch_512.h5\n",
            "Epoch 162/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2805 - acc: 0.9104 - val_loss: 0.5017 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.47518\n",
            "Epoch 163/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2703 - acc: 0.9138 - val_loss: 0.5746 - val_acc: 0.8264\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.47518\n",
            "Epoch 164/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2672 - acc: 0.9145 - val_loss: 0.6298 - val_acc: 0.8005\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.47518\n",
            "Epoch 165/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2756 - acc: 0.9126 - val_loss: 0.4805 - val_acc: 0.8588\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.47518\n",
            "Epoch 166/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2638 - acc: 0.9148 - val_loss: 0.4777 - val_acc: 0.8578\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.47518\n",
            "Epoch 167/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.2718 - acc: 0.9120 - val_loss: 0.5164 - val_acc: 0.8435\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.47518\n",
            "Epoch 168/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2678 - acc: 0.9137 - val_loss: 0.5256 - val_acc: 0.8435\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.47518\n",
            "Epoch 169/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2641 - acc: 0.9150 - val_loss: 0.5175 - val_acc: 0.8390\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.47518\n",
            "Epoch 170/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2614 - acc: 0.9156 - val_loss: 0.5017 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.47518\n",
            "Epoch 171/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2495 - acc: 0.9201 - val_loss: 0.4728 - val_acc: 0.8639\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.47518 to 0.47284, saving model to leafnet_batch_512.h5\n",
            "Epoch 172/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2496 - acc: 0.9198 - val_loss: 0.6908 - val_acc: 0.7927\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.47284\n",
            "Epoch 173/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2523 - acc: 0.9211 - val_loss: 0.5412 - val_acc: 0.8364\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.47284\n",
            "Epoch 174/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2466 - acc: 0.9214 - val_loss: 0.5240 - val_acc: 0.8419\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.47284\n",
            "Epoch 175/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2489 - acc: 0.9186 - val_loss: 0.5019 - val_acc: 0.8471\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.47284\n",
            "Epoch 176/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.2426 - acc: 0.9233 - val_loss: 0.7979 - val_acc: 0.7606\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.47284\n",
            "Epoch 177/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2455 - acc: 0.9202 - val_loss: 0.4849 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.47284\n",
            "Epoch 178/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2365 - acc: 0.9224 - val_loss: 0.5235 - val_acc: 0.8416\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.47284\n",
            "Epoch 179/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2402 - acc: 0.9219 - val_loss: 0.5835 - val_acc: 0.8238\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.47284\n",
            "Epoch 180/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2443 - acc: 0.9206 - val_loss: 0.5166 - val_acc: 0.8426\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.47284\n",
            "Epoch 181/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2402 - acc: 0.9228 - val_loss: 0.5187 - val_acc: 0.8442\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.47284\n",
            "Epoch 182/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.2369 - acc: 0.9218 - val_loss: 0.4876 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.47284\n",
            "Epoch 183/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.2303 - acc: 0.9248 - val_loss: 0.4722 - val_acc: 0.8601\n",
            "\n",
            "Epoch 00183: val_loss improved from 0.47284 to 0.47222, saving model to leafnet_batch_512.h5\n",
            "Epoch 184/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2338 - acc: 0.9217 - val_loss: 0.4634 - val_acc: 0.8568\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.47222 to 0.46341, saving model to leafnet_batch_512.h5\n",
            "Epoch 185/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2233 - acc: 0.9276 - val_loss: 0.5258 - val_acc: 0.8442\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.46341\n",
            "Epoch 186/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2250 - acc: 0.9288 - val_loss: 0.4903 - val_acc: 0.8549\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.46341\n",
            "Epoch 187/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 0.2227 - acc: 0.9288 - val_loss: 0.4868 - val_acc: 0.8552\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.46341\n",
            "Epoch 188/400\n",
            "24692/24692 [==============================] - 11s 449us/step - loss: 0.2220 - acc: 0.9286 - val_loss: 0.4617 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00188: val_loss improved from 0.46341 to 0.46174, saving model to leafnet_batch_512.h5\n",
            "Epoch 189/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.2192 - acc: 0.9270 - val_loss: 0.4772 - val_acc: 0.8536\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.46174\n",
            "Epoch 190/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.2180 - acc: 0.9313 - val_loss: 0.4706 - val_acc: 0.8610\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.46174\n",
            "Epoch 191/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2088 - acc: 0.9335 - val_loss: 0.4548 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.46174 to 0.45476, saving model to leafnet_batch_512.h5\n",
            "Epoch 192/400\n",
            "24692/24692 [==============================] - 11s 457us/step - loss: 0.2117 - acc: 0.9304 - val_loss: 0.4788 - val_acc: 0.8575\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.45476\n",
            "Epoch 193/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2060 - acc: 0.9345 - val_loss: 0.4747 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.45476\n",
            "Epoch 194/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.2143 - acc: 0.9299 - val_loss: 0.4802 - val_acc: 0.8558\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.45476\n",
            "Epoch 195/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2101 - acc: 0.9304 - val_loss: 0.4607 - val_acc: 0.8630\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.45476\n",
            "Epoch 196/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.2017 - acc: 0.9348 - val_loss: 0.4694 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.45476\n",
            "Epoch 197/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2033 - acc: 0.9345 - val_loss: 0.4804 - val_acc: 0.8539\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.45476\n",
            "Epoch 198/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2090 - acc: 0.9314 - val_loss: 0.6546 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.45476\n",
            "Epoch 199/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.2032 - acc: 0.9324 - val_loss: 0.4783 - val_acc: 0.8626\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.45476\n",
            "Epoch 200/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1996 - acc: 0.9353 - val_loss: 0.5456 - val_acc: 0.8286\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.45476\n",
            "Epoch 201/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.2017 - acc: 0.9357 - val_loss: 0.5724 - val_acc: 0.8260\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.45476\n",
            "Epoch 202/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1988 - acc: 0.9367 - val_loss: 0.4653 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.45476\n",
            "Epoch 203/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.2022 - acc: 0.9339 - val_loss: 0.6405 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.45476\n",
            "Epoch 204/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1891 - acc: 0.9395 - val_loss: 0.5149 - val_acc: 0.8465\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.45476\n",
            "Epoch 205/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1915 - acc: 0.9379 - val_loss: 0.5023 - val_acc: 0.8461\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.45476\n",
            "Epoch 206/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1920 - acc: 0.9380 - val_loss: 0.4684 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.45476\n",
            "Epoch 207/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1924 - acc: 0.9367 - val_loss: 0.4766 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.45476\n",
            "Epoch 208/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1881 - acc: 0.9400 - val_loss: 0.4431 - val_acc: 0.8643\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.45476 to 0.44313, saving model to leafnet_batch_512.h5\n",
            "Epoch 209/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1923 - acc: 0.9369 - val_loss: 0.5687 - val_acc: 0.8209\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.44313\n",
            "Epoch 210/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1909 - acc: 0.9378 - val_loss: 0.7285 - val_acc: 0.7771\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.44313\n",
            "Epoch 211/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1874 - acc: 0.9397 - val_loss: 0.6123 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.44313\n",
            "Epoch 212/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1845 - acc: 0.9401 - val_loss: 0.5634 - val_acc: 0.8238\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.44313\n",
            "Epoch 213/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1872 - acc: 0.9390 - val_loss: 0.4461 - val_acc: 0.8682\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.44313\n",
            "Epoch 214/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.1797 - acc: 0.9421 - val_loss: 0.6157 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.44313\n",
            "Epoch 215/400\n",
            "24692/24692 [==============================] - 11s 449us/step - loss: 0.1788 - acc: 0.9414 - val_loss: 0.6134 - val_acc: 0.8179\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.44313\n",
            "Epoch 216/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.1770 - acc: 0.9432 - val_loss: 0.5000 - val_acc: 0.8510\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.44313\n",
            "Epoch 217/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1778 - acc: 0.9422 - val_loss: 0.4736 - val_acc: 0.8614\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.44313\n",
            "Epoch 218/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1769 - acc: 0.9424 - val_loss: 0.5462 - val_acc: 0.8377\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.44313\n",
            "Epoch 219/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1769 - acc: 0.9423 - val_loss: 0.7916 - val_acc: 0.7671\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.44313\n",
            "Epoch 220/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1791 - acc: 0.9422 - val_loss: 0.5989 - val_acc: 0.8202\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.44313\n",
            "Epoch 221/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1731 - acc: 0.9435 - val_loss: 0.4787 - val_acc: 0.8575\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.44313\n",
            "Epoch 222/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1732 - acc: 0.9439 - val_loss: 0.5442 - val_acc: 0.8390\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.44313\n",
            "Epoch 223/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1747 - acc: 0.9424 - val_loss: 0.4520 - val_acc: 0.8678\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.44313\n",
            "Epoch 224/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1691 - acc: 0.9446 - val_loss: 0.4841 - val_acc: 0.8571\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.44313\n",
            "Epoch 225/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 0.1687 - acc: 0.9467 - val_loss: 0.4718 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.44313\n",
            "Epoch 226/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.1649 - acc: 0.9459 - val_loss: 0.9458 - val_acc: 0.7357\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.44313\n",
            "Epoch 227/400\n",
            "24692/24692 [==============================] - 11s 449us/step - loss: 0.1657 - acc: 0.9452 - val_loss: 0.4975 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.44313\n",
            "Epoch 228/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1685 - acc: 0.9459 - val_loss: 0.5111 - val_acc: 0.8465\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.44313\n",
            "Epoch 229/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1629 - acc: 0.9473 - val_loss: 0.4525 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.44313\n",
            "Epoch 230/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.1644 - acc: 0.9453 - val_loss: 0.6527 - val_acc: 0.7924\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.44313\n",
            "Epoch 231/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1660 - acc: 0.9447 - val_loss: 0.4929 - val_acc: 0.8507\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.44313\n",
            "Epoch 232/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.1611 - acc: 0.9482 - val_loss: 0.5300 - val_acc: 0.8435\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.44313\n",
            "Epoch 233/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1634 - acc: 0.9457 - val_loss: 0.4563 - val_acc: 0.8669\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.44313\n",
            "Epoch 234/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1598 - acc: 0.9474 - val_loss: 0.5047 - val_acc: 0.8426\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.44313\n",
            "Epoch 235/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1616 - acc: 0.9465 - val_loss: 0.4516 - val_acc: 0.8656\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.44313\n",
            "Epoch 236/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1589 - acc: 0.9479 - val_loss: 0.4693 - val_acc: 0.8633\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.44313\n",
            "Epoch 237/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1557 - acc: 0.9479 - val_loss: 0.4467 - val_acc: 0.8688\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.44313\n",
            "Epoch 238/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1590 - acc: 0.9485 - val_loss: 0.4933 - val_acc: 0.8510\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.44313\n",
            "Epoch 239/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1530 - acc: 0.9486 - val_loss: 0.4513 - val_acc: 0.8669\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.44313\n",
            "Epoch 240/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1539 - acc: 0.9498 - val_loss: 0.5270 - val_acc: 0.8393\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.44313\n",
            "Epoch 241/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1543 - acc: 0.9502 - val_loss: 0.4758 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.44313\n",
            "Epoch 242/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1487 - acc: 0.9523 - val_loss: 0.4571 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.44313\n",
            "Epoch 243/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 0.1458 - acc: 0.9526 - val_loss: 0.6811 - val_acc: 0.7992\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.44313\n",
            "Epoch 244/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.1510 - acc: 0.9514 - val_loss: 0.5690 - val_acc: 0.8299\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.44313\n",
            "Epoch 245/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1493 - acc: 0.9529 - val_loss: 0.4552 - val_acc: 0.8707\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.44313\n",
            "Epoch 246/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1400 - acc: 0.9524 - val_loss: 0.5843 - val_acc: 0.8118\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.44313\n",
            "Epoch 247/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1491 - acc: 0.9510 - val_loss: 0.6417 - val_acc: 0.7979\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.44313\n",
            "Epoch 248/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1502 - acc: 0.9510 - val_loss: 0.4466 - val_acc: 0.8669\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.44313\n",
            "Epoch 249/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1432 - acc: 0.9530 - val_loss: 0.4531 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.44313\n",
            "Epoch 250/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1453 - acc: 0.9515 - val_loss: 0.6066 - val_acc: 0.8105\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.44313\n",
            "Epoch 251/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1435 - acc: 0.9527 - val_loss: 0.4514 - val_acc: 0.8643\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.44313\n",
            "Epoch 252/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1433 - acc: 0.9540 - val_loss: 0.4355 - val_acc: 0.8743\n",
            "\n",
            "Epoch 00252: val_loss improved from 0.44313 to 0.43554, saving model to leafnet_batch_512.h5\n",
            "Epoch 253/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1416 - acc: 0.9531 - val_loss: 0.4305 - val_acc: 0.8724\n",
            "\n",
            "Epoch 00253: val_loss improved from 0.43554 to 0.43047, saving model to leafnet_batch_512.h5\n",
            "Epoch 254/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1410 - acc: 0.9537 - val_loss: 0.4501 - val_acc: 0.8685\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.43047\n",
            "Epoch 255/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1454 - acc: 0.9524 - val_loss: 0.5258 - val_acc: 0.8416\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.43047\n",
            "Epoch 256/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1406 - acc: 0.9544 - val_loss: 0.5107 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.43047\n",
            "Epoch 257/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1345 - acc: 0.9544 - val_loss: 0.6350 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.43047\n",
            "Epoch 258/400\n",
            "24692/24692 [==============================] - 11s 457us/step - loss: 0.1418 - acc: 0.9544 - val_loss: 0.5505 - val_acc: 0.8335\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.43047\n",
            "Epoch 259/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1373 - acc: 0.9544 - val_loss: 0.5587 - val_acc: 0.8283\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.43047\n",
            "Epoch 260/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1356 - acc: 0.9548 - val_loss: 0.4758 - val_acc: 0.8607\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.43047\n",
            "Epoch 261/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1376 - acc: 0.9536 - val_loss: 0.4274 - val_acc: 0.8691\n",
            "\n",
            "Epoch 00261: val_loss improved from 0.43047 to 0.42738, saving model to leafnet_batch_512.h5\n",
            "Epoch 262/400\n",
            "24692/24692 [==============================] - 11s 457us/step - loss: 0.1342 - acc: 0.9568 - val_loss: 0.7022 - val_acc: 0.7843\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.42738\n",
            "Epoch 263/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1347 - acc: 0.9553 - val_loss: 0.4526 - val_acc: 0.8682\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.42738\n",
            "Epoch 264/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1342 - acc: 0.9555 - val_loss: 0.4439 - val_acc: 0.8717\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.42738\n",
            "Epoch 265/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1297 - acc: 0.9569 - val_loss: 0.5265 - val_acc: 0.8390\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.42738\n",
            "Epoch 266/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1328 - acc: 0.9559 - val_loss: 0.4483 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.42738\n",
            "Epoch 267/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1353 - acc: 0.9549 - val_loss: 0.4499 - val_acc: 0.8678\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.42738\n",
            "Epoch 268/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1315 - acc: 0.9565 - val_loss: 0.4539 - val_acc: 0.8601\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.42738\n",
            "Epoch 269/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1302 - acc: 0.9573 - val_loss: 0.4836 - val_acc: 0.8558\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.42738\n",
            "Epoch 270/400\n",
            "24692/24692 [==============================] - 11s 450us/step - loss: 0.1293 - acc: 0.9563 - val_loss: 0.4508 - val_acc: 0.8639\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.42738\n",
            "Epoch 271/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.1335 - acc: 0.9544 - val_loss: 0.5654 - val_acc: 0.8332\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.42738\n",
            "Epoch 272/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1318 - acc: 0.9563 - val_loss: 0.4192 - val_acc: 0.8779\n",
            "\n",
            "Epoch 00272: val_loss improved from 0.42738 to 0.41921, saving model to leafnet_batch_512.h5\n",
            "Epoch 273/400\n",
            "24692/24692 [==============================] - 11s 457us/step - loss: 0.1375 - acc: 0.9529 - val_loss: 0.4389 - val_acc: 0.8714\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.41921\n",
            "Epoch 274/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1289 - acc: 0.9577 - val_loss: 0.4525 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.41921\n",
            "Epoch 275/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1299 - acc: 0.9579 - val_loss: 0.4667 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.41921\n",
            "Epoch 276/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1275 - acc: 0.9559 - val_loss: 0.4976 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.41921\n",
            "Epoch 277/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1234 - acc: 0.9595 - val_loss: 0.4367 - val_acc: 0.8704\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.41921\n",
            "Epoch 278/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1266 - acc: 0.9602 - val_loss: 0.4402 - val_acc: 0.8720\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.41921\n",
            "Epoch 279/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1253 - acc: 0.9578 - val_loss: 0.6528 - val_acc: 0.8147\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.41921\n",
            "Epoch 280/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1219 - acc: 0.9587 - val_loss: 0.4454 - val_acc: 0.8695\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.41921\n",
            "Epoch 281/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1227 - acc: 0.9587 - val_loss: 0.4570 - val_acc: 0.8669\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.41921\n",
            "Epoch 282/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1244 - acc: 0.9574 - val_loss: 0.9353 - val_acc: 0.7438\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.41921\n",
            "Epoch 283/400\n",
            "24692/24692 [==============================] - 11s 452us/step - loss: 0.1216 - acc: 0.9600 - val_loss: 0.4959 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.41921\n",
            "Epoch 284/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1208 - acc: 0.9589 - val_loss: 0.8689 - val_acc: 0.7486\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.41921\n",
            "Epoch 285/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1168 - acc: 0.9616 - val_loss: 0.4330 - val_acc: 0.8727\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.41921\n",
            "Epoch 286/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1181 - acc: 0.9618 - val_loss: 0.4577 - val_acc: 0.8665\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.41921\n",
            "Epoch 287/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1160 - acc: 0.9618 - val_loss: 0.5058 - val_acc: 0.8507\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.41921\n",
            "Epoch 288/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1215 - acc: 0.9595 - val_loss: 0.4550 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.41921\n",
            "Epoch 289/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1187 - acc: 0.9597 - val_loss: 0.4249 - val_acc: 0.8776\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.41921\n",
            "Epoch 290/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1169 - acc: 0.9594 - val_loss: 0.4780 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.41921\n",
            "Epoch 291/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1155 - acc: 0.9609 - val_loss: 0.4792 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.41921\n",
            "Epoch 292/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1140 - acc: 0.9611 - val_loss: 0.4410 - val_acc: 0.8740\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.41921\n",
            "Epoch 293/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1153 - acc: 0.9612 - val_loss: 0.4361 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.41921\n",
            "Epoch 294/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1108 - acc: 0.9635 - val_loss: 0.4450 - val_acc: 0.8720\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.41921\n",
            "Epoch 295/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1139 - acc: 0.9625 - val_loss: 0.9629 - val_acc: 0.7434\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.41921\n",
            "Epoch 296/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1180 - acc: 0.9608 - val_loss: 0.5150 - val_acc: 0.8465\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.41921\n",
            "Epoch 297/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1125 - acc: 0.9626 - val_loss: 0.4651 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.41921\n",
            "Epoch 298/400\n",
            "24692/24692 [==============================] - 11s 447us/step - loss: 0.1139 - acc: 0.9613 - val_loss: 1.2595 - val_acc: 0.6884\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.41921\n",
            "Epoch 299/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1151 - acc: 0.9618 - val_loss: 1.2927 - val_acc: 0.6725\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.41921\n",
            "Epoch 300/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1119 - acc: 0.9631 - val_loss: 0.4574 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.41921\n",
            "Epoch 301/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1155 - acc: 0.9614 - val_loss: 0.5386 - val_acc: 0.8406\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.41921\n",
            "Epoch 302/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1112 - acc: 0.9627 - val_loss: 0.4365 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.41921\n",
            "Epoch 303/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1120 - acc: 0.9629 - val_loss: 0.4497 - val_acc: 0.8633\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.41921\n",
            "Epoch 304/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1105 - acc: 0.9632 - val_loss: 0.4085 - val_acc: 0.8782\n",
            "\n",
            "Epoch 00304: val_loss improved from 0.41921 to 0.40852, saving model to leafnet_batch_512.h5\n",
            "Epoch 305/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1082 - acc: 0.9639 - val_loss: 0.5181 - val_acc: 0.8406\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.40852\n",
            "Epoch 306/400\n",
            "24692/24692 [==============================] - 11s 457us/step - loss: 0.1083 - acc: 0.9646 - val_loss: 0.4217 - val_acc: 0.8808\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.40852\n",
            "Epoch 307/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1118 - acc: 0.9621 - val_loss: 0.5723 - val_acc: 0.8273\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.40852\n",
            "Epoch 308/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1064 - acc: 0.9649 - val_loss: 0.4483 - val_acc: 0.8695\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.40852\n",
            "Epoch 309/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1037 - acc: 0.9657 - val_loss: 0.4935 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.40852\n",
            "Epoch 310/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1121 - acc: 0.9624 - val_loss: 0.5168 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.40852\n",
            "Epoch 311/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1064 - acc: 0.9645 - val_loss: 0.4307 - val_acc: 0.8753\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.40852\n",
            "Epoch 312/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1101 - acc: 0.9643 - val_loss: 0.4459 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.40852\n",
            "Epoch 313/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1099 - acc: 0.9626 - val_loss: 0.4542 - val_acc: 0.8701\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.40852\n",
            "Epoch 314/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1023 - acc: 0.9668 - val_loss: 0.4366 - val_acc: 0.8746\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.40852\n",
            "Epoch 315/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1068 - acc: 0.9643 - val_loss: 0.9376 - val_acc: 0.7434\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.40852\n",
            "Epoch 316/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1040 - acc: 0.9661 - val_loss: 0.4667 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.40852\n",
            "Epoch 317/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1069 - acc: 0.9640 - val_loss: 0.4762 - val_acc: 0.8626\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.40852\n",
            "Epoch 318/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1033 - acc: 0.9655 - val_loss: 0.4308 - val_acc: 0.8727\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.40852\n",
            "Epoch 319/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1053 - acc: 0.9646 - val_loss: 0.5640 - val_acc: 0.8377\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.40852\n",
            "Epoch 320/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1024 - acc: 0.9652 - val_loss: 0.5316 - val_acc: 0.8426\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.40852\n",
            "Epoch 321/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.1043 - acc: 0.9661 - val_loss: 0.4248 - val_acc: 0.8769\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.40852\n",
            "Epoch 322/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1019 - acc: 0.9668 - val_loss: 0.4940 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.40852\n",
            "Epoch 323/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1016 - acc: 0.9657 - val_loss: 0.4648 - val_acc: 0.8665\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.40852\n",
            "Epoch 324/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1013 - acc: 0.9661 - val_loss: 0.4531 - val_acc: 0.8685\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.40852\n",
            "Epoch 325/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.1011 - acc: 0.9658 - val_loss: 0.5766 - val_acc: 0.8270\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.40852\n",
            "Epoch 326/400\n",
            "24692/24692 [==============================] - 11s 449us/step - loss: 0.1047 - acc: 0.9647 - val_loss: 0.4309 - val_acc: 0.8740\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.40852\n",
            "Epoch 327/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0986 - acc: 0.9655 - val_loss: 0.4380 - val_acc: 0.8737\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.40852\n",
            "Epoch 328/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0989 - acc: 0.9664 - val_loss: 0.4223 - val_acc: 0.8814\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.40852\n",
            "Epoch 329/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0962 - acc: 0.9683 - val_loss: 0.4373 - val_acc: 0.8753\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.40852\n",
            "Epoch 330/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1010 - acc: 0.9658 - val_loss: 0.4444 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.40852\n",
            "Epoch 331/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0981 - acc: 0.9671 - val_loss: 0.4885 - val_acc: 0.8562\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.40852\n",
            "Epoch 332/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.1032 - acc: 0.9649 - val_loss: 0.5425 - val_acc: 0.8458\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.40852\n",
            "Epoch 333/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0972 - acc: 0.9672 - val_loss: 0.4508 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.40852\n",
            "Epoch 334/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.1034 - acc: 0.9635 - val_loss: 0.5232 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.40852\n",
            "Epoch 335/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0950 - acc: 0.9676 - val_loss: 0.6482 - val_acc: 0.8192\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.40852\n",
            "Epoch 336/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0992 - acc: 0.9656 - val_loss: 0.9091 - val_acc: 0.7564\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.40852\n",
            "Epoch 337/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0954 - acc: 0.9687 - val_loss: 0.4453 - val_acc: 0.8669\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.40852\n",
            "Epoch 338/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.0955 - acc: 0.9686 - val_loss: 0.9382 - val_acc: 0.7499\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.40852\n",
            "Epoch 339/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.0958 - acc: 0.9672 - val_loss: 0.5980 - val_acc: 0.8345\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.40852\n",
            "Epoch 340/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0971 - acc: 0.9687 - val_loss: 0.5892 - val_acc: 0.8283\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.40852\n",
            "Epoch 341/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0970 - acc: 0.9676 - val_loss: 0.4245 - val_acc: 0.8743\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.40852\n",
            "Epoch 342/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0925 - acc: 0.9704 - val_loss: 0.4614 - val_acc: 0.8691\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.40852\n",
            "Epoch 343/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.0925 - acc: 0.9679 - val_loss: 0.4642 - val_acc: 0.8678\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.40852\n",
            "Epoch 344/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0986 - acc: 0.9672 - val_loss: 0.8327 - val_acc: 0.7723\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.40852\n",
            "Epoch 345/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.1012 - acc: 0.9655 - val_loss: 0.4674 - val_acc: 0.8662\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.40852\n",
            "Epoch 346/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0989 - acc: 0.9659 - val_loss: 1.1650 - val_acc: 0.7107\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.40852\n",
            "Epoch 347/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0926 - acc: 0.9690 - val_loss: 0.5062 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.40852\n",
            "Epoch 348/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0954 - acc: 0.9655 - val_loss: 0.4082 - val_acc: 0.8831\n",
            "\n",
            "Epoch 00348: val_loss improved from 0.40852 to 0.40819, saving model to leafnet_batch_512.h5\n",
            "Epoch 349/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0938 - acc: 0.9676 - val_loss: 0.8831 - val_acc: 0.7671\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.40819\n",
            "Epoch 350/400\n",
            "24692/24692 [==============================] - 11s 458us/step - loss: 0.0965 - acc: 0.9669 - val_loss: 0.5407 - val_acc: 0.8422\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.40819\n",
            "Epoch 351/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0900 - acc: 0.9693 - val_loss: 0.4542 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.40819\n",
            "Epoch 352/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0913 - acc: 0.9693 - val_loss: 0.6330 - val_acc: 0.8264\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.40819\n",
            "Epoch 353/400\n",
            "24692/24692 [==============================] - 11s 449us/step - loss: 0.0957 - acc: 0.9676 - val_loss: 0.4439 - val_acc: 0.8727\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.40819\n",
            "Epoch 354/400\n",
            "24692/24692 [==============================] - 11s 451us/step - loss: 0.0903 - acc: 0.9689 - val_loss: 0.4220 - val_acc: 0.8811\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.40819\n",
            "Epoch 355/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0901 - acc: 0.9699 - val_loss: 0.5293 - val_acc: 0.8581\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.40819\n",
            "Epoch 356/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0913 - acc: 0.9677 - val_loss: 0.9976 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.40819\n",
            "Epoch 357/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.0943 - acc: 0.9673 - val_loss: 0.5548 - val_acc: 0.8390\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.40819\n",
            "Epoch 358/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0926 - acc: 0.9684 - val_loss: 0.5481 - val_acc: 0.8335\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.40819\n",
            "Epoch 359/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0913 - acc: 0.9688 - val_loss: 0.4276 - val_acc: 0.8727\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.40819\n",
            "Epoch 360/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0888 - acc: 0.9700 - val_loss: 0.4334 - val_acc: 0.8756\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.40819\n",
            "Epoch 361/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0904 - acc: 0.9689 - val_loss: 0.6449 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.40819\n",
            "Epoch 362/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0912 - acc: 0.9696 - val_loss: 0.4464 - val_acc: 0.8724\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.40819\n",
            "Epoch 363/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.0906 - acc: 0.9688 - val_loss: 0.4632 - val_acc: 0.8717\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.40819\n",
            "Epoch 364/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0890 - acc: 0.9700 - val_loss: 0.4584 - val_acc: 0.8707\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.40819\n",
            "Epoch 365/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0908 - acc: 0.9691 - val_loss: 0.7190 - val_acc: 0.7956\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.40819\n",
            "Epoch 366/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.0885 - acc: 0.9703 - val_loss: 0.6462 - val_acc: 0.8196\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.40819\n",
            "Epoch 367/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0834 - acc: 0.9724 - val_loss: 0.4554 - val_acc: 0.8643\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.40819\n",
            "Epoch 368/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0904 - acc: 0.9689 - val_loss: 0.6170 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.40819\n",
            "Epoch 369/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0843 - acc: 0.9715 - val_loss: 0.4439 - val_acc: 0.8727\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.40819\n",
            "Epoch 370/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.0871 - acc: 0.9709 - val_loss: 0.4993 - val_acc: 0.8562\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.40819\n",
            "Epoch 371/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.0860 - acc: 0.9698 - val_loss: 0.7447 - val_acc: 0.7862\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.40819\n",
            "Epoch 372/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.0893 - acc: 0.9689 - val_loss: 0.5326 - val_acc: 0.8419\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.40819\n",
            "Epoch 373/400\n",
            "24692/24692 [==============================] - 11s 456us/step - loss: 0.0851 - acc: 0.9702 - val_loss: 0.4282 - val_acc: 0.8779\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.40819\n",
            "Epoch 374/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0801 - acc: 0.9731 - val_loss: 0.5587 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.40819\n",
            "Epoch 375/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0816 - acc: 0.9729 - val_loss: 0.4546 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.40819\n",
            "Epoch 376/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0828 - acc: 0.9716 - val_loss: 0.4488 - val_acc: 0.8691\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.40819\n",
            "Epoch 377/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0830 - acc: 0.9715 - val_loss: 0.4462 - val_acc: 0.8701\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.40819\n",
            "Epoch 378/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0885 - acc: 0.9694 - val_loss: 0.4440 - val_acc: 0.8756\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.40819\n",
            "Epoch 379/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0840 - acc: 0.9716 - val_loss: 0.7233 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.40819\n",
            "Epoch 380/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.0842 - acc: 0.9720 - val_loss: 0.6080 - val_acc: 0.8238\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.40819\n",
            "Epoch 381/400\n",
            "24692/24692 [==============================] - 11s 448us/step - loss: 0.0868 - acc: 0.9701 - val_loss: 0.4532 - val_acc: 0.8704\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.40819\n",
            "Epoch 382/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0846 - acc: 0.9702 - val_loss: 0.5061 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.40819\n",
            "Epoch 383/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0847 - acc: 0.9717 - val_loss: 0.6859 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.40819\n",
            "Epoch 384/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0836 - acc: 0.9706 - val_loss: 0.4633 - val_acc: 0.8669\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.40819\n",
            "Epoch 385/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0856 - acc: 0.9708 - val_loss: 0.4254 - val_acc: 0.8808\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.40819\n",
            "Epoch 386/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0799 - acc: 0.9731 - val_loss: 0.4838 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.40819\n",
            "Epoch 387/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0837 - acc: 0.9720 - val_loss: 0.5343 - val_acc: 0.8448\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.40819\n",
            "Epoch 388/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.0851 - acc: 0.9710 - val_loss: 1.3739 - val_acc: 0.6699\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.40819\n",
            "Epoch 389/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0801 - acc: 0.9718 - val_loss: 0.4773 - val_acc: 0.8662\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.40819\n",
            "Epoch 390/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0815 - acc: 0.9720 - val_loss: 0.5033 - val_acc: 0.8607\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.40819\n",
            "Epoch 391/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0805 - acc: 0.9722 - val_loss: 0.4423 - val_acc: 0.8724\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.40819\n",
            "Epoch 392/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0835 - acc: 0.9708 - val_loss: 0.6118 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.40819\n",
            "Epoch 393/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0822 - acc: 0.9711 - val_loss: 0.6369 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.40819\n",
            "Epoch 394/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0824 - acc: 0.9719 - val_loss: 0.4441 - val_acc: 0.8743\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.40819\n",
            "Epoch 395/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0812 - acc: 0.9724 - val_loss: 0.4248 - val_acc: 0.8743\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.40819\n",
            "Epoch 396/400\n",
            "24692/24692 [==============================] - 11s 453us/step - loss: 0.0802 - acc: 0.9725 - val_loss: 0.5382 - val_acc: 0.8409\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.40819\n",
            "Epoch 397/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0776 - acc: 0.9731 - val_loss: 0.5683 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.40819\n",
            "Epoch 398/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0787 - acc: 0.9735 - val_loss: 0.4547 - val_acc: 0.8727\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.40819\n",
            "Epoch 399/400\n",
            "24692/24692 [==============================] - 11s 454us/step - loss: 0.0831 - acc: 0.9702 - val_loss: 0.5042 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.40819\n",
            "Epoch 400/400\n",
            "24692/24692 [==============================] - 11s 455us/step - loss: 0.0804 - acc: 0.9717 - val_loss: 0.4367 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.40819\n",
            "Traing finished.\n",
            "Loading the best model...\n",
            "Best Model loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F1nZNKFpeKcx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "metadata": {
        "id": "joT_5pTFXS0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "63deb549-1fda-4e18-eb60-24565cada167"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3087/3087 [==============================] - 1s 404us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38075798154263424, 0.8875931325104]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "jiAKhxZdeRhg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Examination/visualization of results"
      ]
    },
    {
      "metadata": {
        "id": "T5tWPJD8gwLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Results are examined/visualized by the following steps:\n",
        "\n",
        "1. Import packages\n",
        "2. Plot train/dev accuracy and loss"
      ]
    },
    {
      "metadata": {
        "id": "6-xSZk2_eZzb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Import packages"
      ]
    },
    {
      "metadata": {
        "id": "sobfMA2leb86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a1YIjgw7ec4p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Plot train/dev accuracy and loss"
      ]
    },
    {
      "metadata": {
        "id": "f-QqzBSAw7EW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "3ad8e51c-a7e4-4533-aaed-be3e0f15264a"
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(results.history['acc'])\n",
        "plt.plot(results.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(results.history['loss'])\n",
        "plt.plot(results.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhuFNRESkMP8f58W3dXbxrl4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd79d3c08d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFnCAYAAACcvYGMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvqVtMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd79d32a198>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ArtG7MlJiWti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Prediction dataset and confusion matrix\n",
        "\n",
        "Number of examples per class\n",
        "\n",
        "- Histogram of number of examples per class\n",
        "- regression: errors on number of examples (and their correlation)\n",
        "\n",
        "Fehleranzahl nach field/lab?\n",
        "\n",
        "top Fehler-Klassen / top richtige Klassen --> kleine Blätter bei 64x64\n",
        "\n",
        "Heatmaps\n",
        "\n",
        "Zukunft: Mehr computational power, höhere Auflösung (kleine Blätter)"
      ]
    },
    {
      "metadata": {
        "id": "0qhbPpPfxuP2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Terminate the kernel and free memory resources"
      ]
    },
    {
      "metadata": {
        "id": "EzS-wFqgxTmD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}